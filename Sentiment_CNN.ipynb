{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOPvXZb9ke3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ac402f-9d91-4322-d503-9e478053bcd6"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = 'gdrive/My Drive/Deep Learning/sentiment analysis/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9bbk4PHlMTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f0942b9f-5b33-4c98-88c0-886ebec28736"
      },
      "source": [
        "!pip install pytreebank"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytreebank\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp36-none-any.whl size=37072 sha256=a7fdfa82cd065cf1c2fe65cbdb3279cbfddebf50f5193f826dcd9a5fbf4be4dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_cUpuWulnjY"
      },
      "source": [
        "import pytreebank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgTJg7LBlWNI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "11f35789-aeb4-4d45-e3e6-ced8574907ed"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "out_path = os.path.join(dataset_path,sys.path[0], 'sst_{}.txt')\n",
        "print(out_path)\n",
        "dataset = pytreebank.load_sst('./raw_data')\n",
        "# Store train, dev and test in separate files\n",
        "for category in ['train', 'test', 'dev']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][1],\n",
        "                item.to_labeled_lines()[0][0] + 1,\n",
        "            ))\n",
        "# Print the length of the training set\n",
        "print(len(dataset['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/Deep Learning/sentiment analysis/sst_{}.txt\n",
            "8544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DSYUsVXldQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "b4ecef84-a29e-4bde-fda2-5156f004952b"
      },
      "source": [
        "import pandas as pd\n",
        "# Read train data\n",
        "for file in ['sst_train','sst_test','sst_dev']:\n",
        "  print(file)\n",
        "  df = pd.read_csv(dataset_path+file+'.txt', sep='\\t', header=None, names=['text', 'truth'])\n",
        "  #df['truth'] = df['truth'].str.replace('__label__', '')\n",
        "  df['truth'] = df['truth'].astype(int).astype('category')\n",
        "  print(df.shape[0])\n",
        "  print(df.head())\n",
        "  df.to_csv(dataset_path + file+'.csv',index = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sst_train\n",
            "8544\n",
            "                                                text truth\n",
            "0  The Rock is destined to be the 21st Century 's...     4\n",
            "1  The gorgeously elaborate continuation of `` Th...     5\n",
            "2  Singer/composer Bryan Adams contributes a slew...     4\n",
            "3  You 'd think by now America would have had eno...     3\n",
            "4               Yet the act is still charming here .     4\n",
            "sst_test\n",
            "2210\n",
            "                                                text truth\n",
            "0                     Effective but too-tepid biopic     3\n",
            "1  If you sometimes like to go to the movies to h...     4\n",
            "2  Emerges as something rare , an issue movie tha...     5\n",
            "3  The film provides some great insight into the ...     3\n",
            "4  Offers that rare combination of entertainment ...     5\n",
            "sst_dev\n",
            "1101\n",
            "                                                text truth\n",
            "0  It 's a lovely film with lovely performances b...     4\n",
            "1  No one goes unindicted here , which is probabl...     3\n",
            "2  And if you 're not nearly moved to tears by a ...     4\n",
            "3                   A warm , funny , engaging film .     5\n",
            "4  Uses sharp humor and insight into human nature...     5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnC-9IsUmO38"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 1234\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "import nltk\n",
        "\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#import pyprind\n",
        "%matplotlib inline  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVL0BlKXwJMq"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbiFCek_xGw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ded1a92-e15e-4c9b-acb5-eb1209b361fe"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "print(\"Cuda Status on system is {}\".format(is_cuda))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda Status on system is True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTNoWz2Y8NKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273dee38-6d9f-477f-abc2-a255d8f4eaf7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNyBbhPxaqA"
      },
      "source": [
        "Filters were of sizes 3, 4 and 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQoThHm5xZjP"
      },
      "source": [
        "FILTER_SIZES = [3,4,5]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTznVjvhyFAo"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "\n",
        "def tokenizer(text):\n",
        "  token = [t.text for t in spacy_en.tokenizer(text)]\n",
        "  if len(token) < FILTER_SIZES[-1]:\n",
        "      for i in range(0, FILTER_SIZES[-1] - len(token)):\n",
        "          token.append('<PAD>')\n",
        "  return token"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMaa9NEyNxr"
      },
      "source": [
        "TEXT = data.Field(tokenize = tokenizer, batch_first = True)\n",
        "LABEL = data.LabelField(dtype = torch.long)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufKFV9CpyTUQ"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Owx8gK4yjDb"
      },
      "source": [
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path=dataset_path, train=\"sst_train.csv\", \n",
        "    validation=\"sst_dev.csv\", test=\"sst_test.csv\",format=\"csv\", skip_header=True, \n",
        "    fields=[('text', TEXT), ('truth', LABEL)]\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hP9lEyBzJqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f806607-41bc-41fb-ab08-34515542fd93"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of valid examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 8544\n",
            "Number of valid examples: 1101\n",
            "Number of testing examples: 2210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTVhELcKzOMJ"
      },
      "source": [
        "TEXT.build_vocab(train_data, vectors=torchtext.vocab.Vectors(dataset_path+\"glove.840B.300d.txt\"), \n",
        "                 max_size=25000,unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqSEVglx9Xpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef68ef73-7e7e-4e7a-bdf8-4f0a3864b5c3"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 17166\n",
            "Unique tokens in LABEL vocabulary: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT68SzzDVBEN"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), sort_key=lambda x: len(x.text),\n",
        "    batch_size=BATCH_SIZE,sort_within_batch = True,\n",
        "    device=device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFyrW8Aqsmxj"
      },
      "source": [
        "Following is the CNN architecture. Filters of sizes 3, 4 and 5 are applied on the text along with max pooling. At the end, the information is passed on to a fully connected layer, which then outputs scores through softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cksRo5qXBsrb"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "                \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5hlHUUxtEt7"
      },
      "source": [
        "The best set of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthLuDFpCGIx"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 5\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFbraQ4nzk8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba7c6f7-0967-4c0a-83eb-8c6b9e061be8"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 5,511,605 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy4K7ZwSCNYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5367652-76cc-4755-a31c-54123125cda1"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
              "        ...,\n",
              "        [ 0.0495, -0.2737, -0.2819,  ..., -0.2686,  0.5445,  0.1999],\n",
              "        [ 0.8430, -0.0559, -0.0837,  ...,  0.9208, -0.2708, -0.4322],\n",
              "        [ 0.4218,  0.2891,  0.6224,  ..., -0.0994, -0.3216, -0.2066]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puOZniDaSlzn"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRFkkTxyCTrc"
      },
      "source": [
        "#class_weights = torch.tensor([1.0, 15.0]).cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)#,weight_decay=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-PPtDr3CZTF"
      },
      "source": [
        "def binary_accuracy(preds1, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    preds, ind= torch.max(F.softmax(preds1),1)\n",
        "    correct = (ind == y).float()\n",
        "    acc = correct.sum()/float(len(correct))\n",
        "    return acc"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djr3YbaHC6Eg"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.truth)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.truth)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fESTz3KxDGmh"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.truth)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.truth)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfB6_4l4DVgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34d9576-6f85-4453-dbfe-16ef9e85aae9"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 1.562 | Train Acc: 28.68% | Val. Loss: 1.521 | Val. Acc: 36.08% |\n",
            "| Epoch: 02 | Train Loss: 1.485 | Train Acc: 36.10% | Val. Loss: 1.466 | Val. Acc: 37.91% |\n",
            "| Epoch: 03 | Train Loss: 1.415 | Train Acc: 40.75% | Val. Loss: 1.413 | Val. Acc: 39.46% |\n",
            "| Epoch: 04 | Train Loss: 1.349 | Train Acc: 43.90% | Val. Loss: 1.369 | Val. Acc: 40.59% |\n",
            "| Epoch: 05 | Train Loss: 1.285 | Train Acc: 47.76% | Val. Loss: 1.339 | Val. Acc: 40.85% |\n",
            "| Epoch: 06 | Train Loss: 1.238 | Train Acc: 49.21% | Val. Loss: 1.318 | Val. Acc: 41.89% |\n",
            "| Epoch: 07 | Train Loss: 1.193 | Train Acc: 51.98% | Val. Loss: 1.302 | Val. Acc: 43.11% |\n",
            "| Epoch: 08 | Train Loss: 1.151 | Train Acc: 54.52% | Val. Loss: 1.290 | Val. Acc: 42.68% |\n",
            "| Epoch: 09 | Train Loss: 1.118 | Train Acc: 56.61% | Val. Loss: 1.280 | Val. Acc: 43.64% |\n",
            "| Epoch: 10 | Train Loss: 1.073 | Train Acc: 58.80% | Val. Loss: 1.277 | Val. Acc: 44.33% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER9FN6pJEYA3"
      },
      "source": [
        "test_add_notan = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db59ayP8DeKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829d2877-efb9-4df6-f695-98dcb8bdaee3"
      },
      "source": [
        "\n",
        "#model.load_state_dict(torch.load(dataset_path + 'tut1-model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "#test_add_notan.append(test_acc*100)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.246 | Test Acc: 46.05%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Q_9_E22E8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5bbcc89-228b-4e71-faa4-ad7adaae355a"
      },
      "source": [
        "test_add_notan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[46.919642857142854, 48.16964285714286, 46.517857142857146, 47.58928571428571]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 514
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3p756ziEhsj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92a52e09-c9c0-4d8b-eab0-ad0376e78ad8"
      },
      "source": [
        "sum(test_norm_tanh)/len(test_norm_tanh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.27678571428571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 356
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X0L8wOFGOcO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3189be9e-1dc2-4e41-fe5f-0d4109b96321"
      },
      "source": [
        "sum(test_1)/len(test_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.69642857142857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjC5kpfsNHkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4a1c809-6859-4034-8088-870c26a31ac3"
      },
      "source": [
        "sum(test)/len(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.84821428571429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 357
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgPCCa5JX3PG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a19b6e99-26d9-48d4-aefc-4c26cf537ea4"
      },
      "source": [
        "test_tanh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[46.964285714285715,\n",
              " 47.767857142857146,\n",
              " 48.16964285714286,\n",
              " 47.5,\n",
              " 47.00892857142857,\n",
              " 46.60714285714286]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 459
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aks-M62GUmeD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bff7b354-e060-4322-c285-17de40fa7aa4"
      },
      "source": [
        "sum(test_tanh[:-1])/len(test_tanh[:-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.482142857142854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 460
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCUj-yerUpHZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecbada68-f790-44dc-c6db-aca68a0cbbe3"
      },
      "source": [
        "sum(test_add)/len(test_add)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.481142857142856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 458
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c2IlzRaVd5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f88959ec-d77e-4d37-8239-c55e9a2704e9"
      },
      "source": [
        "sum(test_add_notan)/len(test_add_notan)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.29910714285714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 515
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSZim36SZpvA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}